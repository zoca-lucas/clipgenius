# ğŸ¬ ClipGenius - Gerador AutomÃ¡tico de Cortes com IA

Transforme vÃ­deos longos do YouTube em cortes virais automaticamente usando IA.

**ğŸ’° 100% GRATUITO - Usa Ollama (IA local) + Whisper**

![ClipGenius Demo](https://via.placeholder.com/800x400?text=ClipGenius+Demo)

## âœ¨ Funcionalidades

- ğŸ“¥ **Download automÃ¡tico** do YouTube via yt-dlp
- ğŸ™ï¸ **TranscriÃ§Ã£o com Whisper** - legendas sincronizadas (gratuito, local)
- ğŸ¤– **AnÃ¡lise com Ollama** - IA local gratuita identifica os melhores momentos
- âœ‚ï¸ **15 cortes automÃ¡ticos** - formato 9:16 para Shorts/Reels
- â­ **Notas de viralidade** - cada corte recebe nota de 0-10
- ğŸ“ **Legendas estilizadas** - burn-in automÃ¡tico

## ğŸ› ï¸ Tecnologias

| Componente | Tecnologia | Custo |
|------------|------------|-------|
| Backend | Python + FastAPI | Gratuito |
| Frontend | Next.js 14 + React | Gratuito |
| TranscriÃ§Ã£o | OpenAI Whisper (local) | **Gratuito** |
| AnÃ¡lise IA | **Ollama (local)** | **Gratuito** |
| VÃ­deo | FFmpeg | Gratuito |
| Download | yt-dlp | Gratuito |
| Database | SQLite | Gratuito |

## ğŸ“‹ PrÃ©-requisitos

- Python 3.9+
- Node.js 18+
- FFmpeg instalado no sistema
- **Ollama** instalado (IA local gratuita)

### Instalando FFmpeg

**macOS:**
```bash
brew install ffmpeg
```

**Ubuntu/Debian:**
```bash
sudo apt update && sudo apt install ffmpeg
```

**Windows:**
```bash
winget install ffmpeg
```

### ğŸ¤– Instalando Ollama (IMPORTANTE!)

Ollama Ã© a IA local que analisa os vÃ­deos. 100% gratuito e privado.

**1. Instale o Ollama:**
```bash
# macOS / Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Ou baixe em: https://ollama.ai/download
```

**2. Baixe um modelo:**
```bash
# Recomendado para Mac (rÃ¡pido e bom)
ollama pull llama3.2

# Alternativas:
# ollama pull mistral     # Mais leve
# ollama pull llama3.1    # Mais potente
```

**3. Verifique se estÃ¡ funcionando:**
```bash
ollama list
# Deve mostrar: llama3.2:latest
```

## ğŸš€ InstalaÃ§Ã£o

### 1. VÃ¡ para o diretÃ³rio

```bash
cd ~/clipgenius
```

### 2. Configure o Backend

```bash
cd backend

# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/macOS
# ou: venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 3. Configure o Frontend

```bash
cd ../frontend

# Instalar dependÃªncias
npm install
```

## ğŸ¯ Como Usar

### 1. Inicie o Ollama (em um terminal)

```bash
ollama serve
```

### 2. Inicie o Backend (em outro terminal)

```bash
cd ~/clipgenius/backend
source venv/bin/activate
python main.py
```

O backend estarÃ¡ em: http://localhost:8000

### 3. Inicie o Frontend (em outro terminal)

```bash
cd ~/clipgenius/frontend
npm run dev
```

O frontend estarÃ¡ em: http://localhost:3000

### 4. Use o App

1. Acesse http://localhost:3000
2. Cole um link do YouTube
3. Clique em "Gerar Cortes"
4. Aguarde o processamento (alguns minutos)
5. Visualize e baixe seus cortes!

## ğŸ“ Estrutura do Projeto

```
clipgenius/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py              # FastAPI app
â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ models/              # Modelos do banco
â”‚   â”œâ”€â”€ services/            # ServiÃ§os (download, transcriÃ§Ã£o, etc)
â”‚   â”œâ”€â”€ api/                 # Rotas da API
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/             # PÃ¡ginas Next.js
â”‚   â”‚   â”œâ”€â”€ components/      # Componentes React
â”‚   â”‚   â””â”€â”€ lib/             # UtilitÃ¡rios e API client
â”‚   â””â”€â”€ package.json
â””â”€â”€ data/
    â”œâ”€â”€ videos/              # VÃ­deos originais
    â”œâ”€â”€ clips/               # Cortes gerados
    â”œâ”€â”€ audio/               # Ãudios extraÃ­dos
    â””â”€â”€ database.db          # SQLite
```

## ğŸ”§ API Endpoints

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| POST | `/api/projects` | Criar projeto (URL do YouTube) |
| GET | `/api/projects` | Listar projetos |
| GET | `/api/projects/{id}` | Detalhes do projeto |
| GET | `/api/projects/{id}/status` | Status do processamento |
| GET | `/api/projects/{id}/clips` | Listar cortes |
| GET | `/api/clips/{id}/download` | Baixar corte |
| DELETE | `/api/projects/{id}` | Deletar projeto |
| DELETE | `/api/clips/{id}` | Deletar corte |

## âš™ï¸ ConfiguraÃ§Ãµes

### VariÃ¡veis de Ambiente (backend/.env)

```env
# Ollama (IA local gratuita)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Whisper (transcriÃ§Ã£o local)
WHISPER_MODEL=base

# DiretÃ³rios
DATA_DIR=../data
DATABASE_URL=sqlite:///../data/database.db
```

### Modelos do Ollama

| Modelo | RAM | Velocidade | Qualidade |
|--------|-----|------------|-----------|
| mistral | ~4GB | Muito rÃ¡pido | Boa |
| llama3.2 | ~4GB | RÃ¡pido | Muito boa |
| llama3.1 | ~8GB | Moderado | Excelente |
| llama3.1:70b | ~40GB | Lento | MÃ¡xima |

### Modelos do Whisper

| Modelo | VRAM | Velocidade | Qualidade |
|--------|------|------------|-----------|
| tiny | ~1GB | Muito rÃ¡pido | BÃ¡sica |
| base | ~1GB | RÃ¡pido | Boa |
| small | ~2GB | Moderado | Muito boa |
| medium | ~5GB | Lento | Excelente |
| large | ~10GB | Muito lento | MÃ¡xima |

## ğŸ’¡ Dicas

1. **VÃ­deos curtos primeiro**: Teste com vÃ­deos de 5-10 minutos
2. **Modelo Whisper**: Use `base` para equilÃ­brio velocidade/qualidade
3. **Modelo Ollama**: Use `llama3.2` para melhor custo-benefÃ­cio
4. **GPU**: Se tiver NVIDIA, Whisper e Ollama usam CUDA automaticamente
5. **Legendas**: Edite o arquivo .ass se quiser customizar o estilo

## ğŸ’° Custos

| Item | Custo |
|------|-------|
| Ollama | **Gratuito** (local) |
| Whisper | **Gratuito** (local) |
| FFmpeg | **Gratuito** |
| **Total por vÃ­deo** | **R$ 0,00** |

## ğŸ› Troubleshooting

### Ollama nÃ£o estÃ¡ rodando
```bash
# Inicie o Ollama
ollama serve

# Em outro terminal, verifique
curl http://localhost:11434/api/tags
```

### Modelo nÃ£o encontrado
```bash
# Baixe o modelo
ollama pull llama3.2

# Liste modelos disponÃ­veis
ollama list
```

### FFmpeg nÃ£o encontrado
```bash
# Verifique a instalaÃ§Ã£o
ffmpeg -version

# macOS
brew install ffmpeg
```

### Erro de CUDA/GPU
```bash
# Use CPU se nÃ£o tiver GPU NVIDIA
# Ollama e Whisper detectam automaticamente
```

### Processamento muito lento
```bash
# Use modelos menores
# Em backend/.env:
OLLAMA_MODEL=mistral
WHISPER_MODEL=tiny
```

## ğŸ“œ LicenÃ§a

MIT License - Use como quiser para projetos pessoais!

---

**Feito com â¤ï¸ usando Ollama, Whisper e FFmpeg - 100% Gratuito!**
